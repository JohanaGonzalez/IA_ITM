{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n\n## Inteligencia Artificial - IAI84\n### Instituto Tecnológico Metropolitano\n#### Pedro Atencio Ortiz - 2019\n\n\nEn este notebook se aborda el tema de aprendizaje de máquina para clasificación binaria utilizando Regresión Logística:\n1. Propagación hacia adelante (forward propagation)\n2. Función de pérdida\n3. Función de costo\n4. Descenso del gradiente\n5. Predicción\n6. Regresión logística sobre un dataset"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src='res/logistic_regression/graph.png'>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- La regresión logística se puede entender como una regresión lineal, acompañada de una función logística (sigmoide) que permite determinar la clase a la que pertenece un objeto.\n- Solo se aplica en problemas de clasificación binaria lineal."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 1. Propagación hacia adelante (backward propagation)\n<hr>\n## 1.1. Activación lineal (linear activation)\n\nA continuación implementemos la activación lineal definida como:\n\n## <center>$z = W^{T}X+b$</center>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n Use numpy implementation of dot product np.dot(A,B) to multiply W.T and b\n'''\ndef linear_activation(W, b, X):\n    z = None\n    \n    return z",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2 #to be able to verify your result\nnp.random.seed(seed)\nW = np.random.randn(2,1)\nb = np.random.rand()\nX = np.random.randn(2, 3)\n\nprint(linear_activation(W,b,X))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Resultado esperado: __[[ 1.39865165  1.29546477  0.94717078]]__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 1.2. Activación logística (sigmoid activation)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A continuación implementemos la función sigmoide definida como:\n\n## <center>$sig(z)=\\sigma(z) = \\frac{1}{1+e^{-z}}$</center>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\nUse numpy implementation for exponential e^k -- np.exp(k)\n'''\ndef sigmoid(z):\n    '''\n    Returns sigmoid activation for array z\n    \n    Arguments:\n        z -- array of float values.\n    Returns:\n        a -- sigmoid activation.\n    '''\n    a = None \n    \n    return a ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2\nnp.random.seed(seed)\nz = np.random.randn(1,3)\nprint(sigmoid(z))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Valor esperado = __[[ 0.39729283  0.485937    0.10562821]]__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 2. Función de perdida\n\nA continuación implementemos la función de perdida como:\n\n## <center>$loss(y, a)=L(y, a) = -(y.log(a)+(1-y).log(1-a))$</center>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\nUse np.log(a) to implement natural logarithm.\n'''\ndef loss(y, a):\n    '''\n    Logistic loss implementation.\n    \n    Arguments:\n        y -- original labels.\n        a -- predicted labels from forward propagation.\n    '''\n    logloss = None\n    return logloss",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2 #to be able to verify your result\nnp.random.seed(seed)\nW = np.random.randn(2,1)\nb = np.random.rand()\nX = np.random.randn(2, 3)\n\nY = np.array([[1,1,0]]) #original labels for features X\nA = sigmoid(linear_activation(W,b,X)) #forward activation\n\nprint(\"Perdida dato a dato: \", loss(Y, A))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Resultado esperado: __[[ 0.22068428,  0.24198147,  1.27491702]]__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 3. Función de costo\n\nImplementos la función de costo definida como:\n\n## <center>$cost(logloss) = J(logloss) = \\frac{1}{m}\\sum^{i=1}_{m}logloss^{(i)}$</center>\n\npara $m$ ejemplos en el dataset $X$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\nCost is defined as the mean of logistic loss.\n'''\ndef cost(logloss):\n    return None",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "logloss = np.array([[0.22068428,  0.24198147,  1.27491702]])\nprint(\"costo: \", cost(logloss))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Resultado esperado: __0.57919425666666668__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 4. Descenso del gradiente (Gradient Descent)\n\nEl descenso del gradiente implica aplicar la regla de la cadena sobre el grafo de cómputo de la regresión logística y calcular las derivadas parciales de los parámetros $W$ y $b$ respecto a la función de pérdida. Posteriormente el negativo de la derivada de los parámetros se utiliza para actualizar los mismos.\n\n<img src='res/logistic_regression/graph_backward.png' width=700>\n\n<br>\nEl algoritmo vectorizado del descenso del gradiente para la regresión logística es:\n\n<i>\n__FOR__ i=1->epochs:\n\n    //forward propagation\n    Z = W.T*X+b\n    A = sig(Z)\n    \n    //cálculo de los gradientes de los parámetros\n    dz = A-Y\n    dW = (X*dz.T) / m\n    db = sum(dz) / m\n    \n    //cálculo del costo\n    J = cost(loss(Y,A))\n    \n    //actualizacion de parametros\n    W = W - learning_rate*dW\n    b = b - learning_rate*db\n</i>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2\nnp.random.seed(seed)\n\nX = np.random.rand(3,2)\nY = np.array([[0, 1]])\n\nm = X.shape[1]\n\nW = np.array([[0.1], [-0.1], [0.01]])\nb = 0.1\n\nprint(\"m: \", m)\nprint(\"W inicial: \",W)\nprint(\"b inicial: \",b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Utilicemos las funciones previamente diseñadas para completar el siguiente código"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "learning_rate = 0.05\n\nfor i in range(1000): #1000 iteraciones del descenso del gradiente\n    Z = None\n    A = None\n    \n    dz = None\n    dW = None\n    db = None\n    \n    J = None\n\n    W -= None\n    b -= None\n    \n    if(i%100 == 0):\n        print(\"costo: \", J)\n\nprint(\"W actualizado: \",W)\nprint(\"b actualizado: \",b)\nprint(\"costo total: \", J)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__Resultado esperado__: \n\n('costo: ', 0.35961193473699637)\n\n('costo: ', 0.34112286128068314)\n\n('costo: ', 0.32416576026635818)\n\n('costo: ', 0.3085804969161185)\n\n('costo: ', 0.29422579454831238)\n\n('costo: ', 0.28097695742118511)\n\n('costo: ', 0.26872383438602254)\n\n('costo: ', 0.25736901432660603)\n\n('costo: ', 0.24682623751141713)\n\n('costo: ', 0.23701900363846592)\n\n('W actualizado: ', array([[-4.43431392],\n       [-4.63431392],\n       [-4.52431392]]))\n       \n('b actualizado: ', 4.8105808996169204)\n\n('costo total: ', 0.22796765247707698)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 5. Predicción\n\nLa predicción consiste en aplicar forward propagation utilizando los W y b optimizados mediante descenso del gradiente."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predict(W,b,X):\n    z = linear_activation(W,b,X)\n    A = sigmoid(z)\n    return np.round(A)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 6. Regresión Logística sobre un dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from utils import generate_data, visualize, plot_decision_boundary\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.1.Generemos el dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X, Y = generate_data('blobs')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "color= ['red' if y == 1 else 'green' for y in np.squeeze(Y)]\n\nplt.figure(figsize=(7,5))\nplt.scatter(X[:,0], X[:,1], color=color)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Y = Y.reshape(1,len(Y))# reshape Y to satisfy shape (m, 1)\nX = X.T #transpose X to satisfy shape (nx, m)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.2. Inicialicemos los parámetros W y b"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#1. inicilicemos parametros W y b\nm = X.shape[1]\n\nW = np.random.randn(X.shape[0],1)\nb = 0\n\nprint(\"m: \", m)\nprint(\"W inicial: \",W)\nprint(\"b inicial: \",b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6.3. Apliquemos la regresión logística"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#2. Regresion logistica mediante descenso del gradiente\n\nlearning_rate = 0.1\n\nfor i in range(10000): #10000 iteraciones del descenso del gradiente\n    Z = None\n    A = None\n    dz = None\n    dW = None\n    db = None\n    J = None\n    \n    W -= None\n    b -= None\n    \n    if(i%1000 == 0):\n        print(\"costo: \", J)\n\nprint(\"W actualizado: \",W)\nprint(\"b actualizado: \",b)\nprint(\"costo final (error), despues de \",i+1,\" iteraciones: \", J)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(predict(W,b,X))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def visualize_lr(W, b, X, y):\n     \"\"\"\n    Plots a classification boundary for a logistic regression model\n    defined by W and b, using X (inputs) and y (outputs)\n\n    Arguments:\n    data_type -- distribution of dataset {moons,circles,blobs}\n\n    Returns:\n    W -- weights of lr model\n    b -- bias of lr model\n    X -- features\n    y -- labels\n    \"\"\"\n    X = X.T\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z = predict(W,b,np.c_[xx.ravel(), yy.ravel()].T)\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.figure(figsize=(7,5))\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    \n    color= ['black' if y == 1 else 'yellow' for y in np.squeeze(Y)]\n    plt.scatter(X[:, 0], X[:, 1], cmap=plt.cm.Spectral, c=color)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "visualize_lr(W, b, X, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
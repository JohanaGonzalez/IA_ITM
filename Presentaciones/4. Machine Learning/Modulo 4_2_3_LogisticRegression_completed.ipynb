{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2018\n",
    "\n",
    "\n",
    "En este notebook se aborda el tema de aprendizaje de máquina para clasificación binaria utilizando Regresión Logística:\n",
    "1. Propagación hacia adelante (forward propagation)\n",
    "2. Función de pérdida\n",
    "3. Función de costo\n",
    "4. Descenso del gradiente\n",
    "5. Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1. Propagación hacia adelante (backward propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    '''\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(2)\n",
    "z = np.random.randn(1,3)\n",
    "print(sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W.T,X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3],[4,5,6]]).T\n",
    "print(\"X: \",X)\n",
    "\n",
    "Y = np.array([[0, 1]])\n",
    "print(\"Y: \", Y)\n",
    "\n",
    "W = np.array([[0.4], [-0.5], [0.01]])\n",
    "print(\"W: \", W)\n",
    "\n",
    "b = 0.3\n",
    "print(\"b: \", b)\n",
    "\n",
    "A = sigmoid(linear_activation(W, b, X))\n",
    "\n",
    "print(\"forward propagation: \", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 2. Función de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, a):\n",
    "    return -(y * np.log(a) + (1-y) * np.log(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 #to be able to verify your result\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(2,1)\n",
    "b = np.random.rand()\n",
    "X = np.random.randn(2, 3)\n",
    "\n",
    "Y = np.array([[1,1,0]]) #original labels for features X\n",
    "A = sigmoid(linear_activation(W,b,X)) #forward activation\n",
    "\n",
    "print(\"Perdida dato a dato: \", loss(Y, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 3. Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(logloss):\n",
    "    return np.mean(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = np.array([[0.22068428,  0.24198147,  1.27491702]])\n",
    "print(\"costo: \", cost(logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 4. Descenso del gradiente (Gradient Descent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "X = np.random.rand(3,2)\n",
    "Y = np.array([[0, 1]])\n",
    "\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.array([[0.1], [-0.1], [0.01]])\n",
    "b = 0.1\n",
    "\n",
    "print(\"m: \", m)\n",
    "print(\"W inicial: \",W)\n",
    "print(\"b inicial: \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "for i in range(1000): #1000 iteraciones del descenso del gradiente\n",
    "    Z = linear_activation(W,b,X)\n",
    "    A = sigmoid(Z)\n",
    "    dz = A - Y\n",
    "    dW = np.dot(X,dz.T) / m\n",
    "    db = np.sum(dz) / m\n",
    "    J = cost(loss(Y,A))\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if(i%100 == 0):\n",
    "        print(\"costo: \", J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo total: \", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predicción\n",
    "\n",
    "La predicción consiste en aplicar forward propagation utilizando los W y b optimizados mediante descenso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W,b,X):\n",
    "    z = linear_activation(W,b,X)\n",
    "    A = sigmoid(z)\n",
    "    return np.round(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = predict(W,b,X)\n",
    "print(\"predicciones: \",np.round(Y_hat))\n",
    "print(\"clases originales: \", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Regresión Logística sobre un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Utility functions\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(data_type, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generate a binary dataset with distribution data_type\n",
    "\n",
    "    Arguments:\n",
    "    data_type -- distribution of dataset {moons,circles,blobs}\n",
    "\n",
    "    Returns:\n",
    "    X -- features\n",
    "    Y -- labels\n",
    "    \"\"\" \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(200, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_lr(W, b, X, y):\n",
    "    \"\"\"\n",
    "    Plots a classification boundary for a logistic regression model\n",
    "    defined by W and b, using X (inputs) and y (outputs)\n",
    "\n",
    "    Arguments:\n",
    "    data_type -- distribution of dataset {moons,circles,blobs}\n",
    "\n",
    "    Returns:\n",
    "    W -- weights of lr model\n",
    "    b -- bias of lr model\n",
    "    X -- features\n",
    "    y -- labels\n",
    "    \"\"\"\n",
    "    X = X.T\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict(W,b,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('blobs', 1.5)\n",
    "Y = Y.reshape(1,len(Y))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color= ['red' if y == 1 else 'green' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. inicilicemos parametros W y b\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.random.randn(X.shape[0],1)\n",
    "b = 0\n",
    "\n",
    "print(\"m: \", m)\n",
    "print(\"W inicial: \",W)\n",
    "print(\"b inicial: \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primero visualicemos cual seria la clasificacion con valores de W y b aleatorios.\n",
    "'''\n",
    "visualize_lr(W, b, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Regresion logistica mediante descenso del gradiente\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "for i in range(10000): #1000 iteraciones del descenso del gradiente\n",
    "    Z = linear_activation(W,b,X)\n",
    "    A = sigmoid(Z)\n",
    "    dz = A - Y\n",
    "    dW = np.dot(X,dz.T) / m\n",
    "    db = np.sum(dz) / m\n",
    "    J = np.sum(-(Y * np.log(A) + (1-Y)*np.log(1-A))) / m\n",
    "    \n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo: \", J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo final (error), despues de \",i+1,\" iteraciones: \", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(W,b,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_lr(W, b, X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

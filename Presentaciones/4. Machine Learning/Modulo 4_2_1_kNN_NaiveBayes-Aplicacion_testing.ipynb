{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n\n## Inteligencia Artificial - IAI84\n### Instituto Tecnológico Metropolitano\n#### Pedro Atencio Ortiz - 2019\n\nEn este notebook se aborda un ejemplo de aplicación del algoritmo k-NN para la clasificación de imágenes en dos categorías: perros o gatos, utilizando SKLearn. Por otra parte se tratan distintas formas de evaluación."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## k-Vecinos Cercanos (k-NN)\n\nUna aproximación más sofisticada, clasificación k-NN, encuentra un grupo de $k$ objetos en el conjunto de entrenamiento que se encuentran más cerca del objeto de prueba, y asigna una clase al mismo basado en la predominancia de una clase particular en el vecindario.\n<img src=\"res/knn/knn.png\" width=\"400\">\nDados un conjunto de entrenamiento $(X,Y)$ y un objeto de prueba $x_i$, el algoritmo computa la distancia o similaridad entre $x_i$ y todos los objetos de entrenamiento que pertenecen a $(X,Y)$ para determinar la lista de vecinos más cercanos.  Una vez se obtiene dicha lista, $x_i$ se clasifica con la clase de mayor aparición en su vecindario (mayoría de votos). \n<img src=\"res/knn/knn_example.png\" width=\"700\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# Perro o Gato?\n\n<img src=\"res/knn/clasificacion.png\" width=\"500\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## Caracteristicas...\n\nUna forma de enfrentar este problema, es tomar los píxeles como características de las imágenes que se desea clasificar. Esta aproximación es ingenua, ya que en una imagen existe más información que la simple secuencia de los píxeles que la componen. Sin embargo para este caso procedamos de esta manera."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread, imshow\n\ncat_image = imread(\"dogscats_dataset/train/cats/cat.18.jpg\")\nprint(\"image shape: \", cat_image.shape)\n\nf, ax = plt.subplots(1,4, figsize=(20,10))\nax[0].imshow(cat_image, cmap='gray')\nax[0].set_title(\"imagen original\")\nax[1].imshow(cat_image[:,:,0], cmap='gray')\nax[1].set_title(\"matrix R\")\nax[2].imshow(cat_image[:,:,1], cmap='gray')\nax[2].set_title(\"matrix G\")\nax[3].imshow(cat_image[:,:,2], cmap='gray')\nax[3].set_title(\"matrix B\")\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Reshape..."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from skimage.transform import resize\n\ncat_image_resize = resize(cat_image, (64,64))\nprint(\"new shape: \", cat_image_resize.shape)\n\nplt.imshow(cat_image_resize)\nplt.show()\n\n'''\na = np.random.randn(3,3,3)\nprint(a.shape)\nprint(a)\na_flat = a.flatten()\nprint(a_flat.shape)\nprint(a_flat)\n'''\n\n#Transformams la imagen en un vector de (1, 64x64x3) = (1, 12288)\ncat_image_x = cat_image_resize.flatten().reshape(1, 12288)\nprint(\"flattened shape: \",cat_image_x.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from os import listdir\nfrom os.path import isfile\n\n\ndef get_dataset_size(path):\n    cat_files = listdir(path)\n    \n    number_of_images = 0\n    for f in cat_files:\n        if(not(f.startswith(\".\")) and f.endswith(\".jpg\")):\n            number_of_images += 1\n    \n    return number_of_images\n\ndef load_dataset(folder_path, imsize=(64,64,3), class_index=0):\n    \n    folder_files = listdir(folder_path)\n    folder_len = get_dataset_size(folder_path)\n    \n    flattened_size = imsize[0]*imsize[1]*imsize[2]\n    \n    X = np.zeros([folder_len, flattened_size])\n    Y = np.ones([folder_len, 1]) * class_index\n    \n    i = 0\n    for f in folder_files:\n        if(not(f.startswith(\".\")) and f.endswith(\".jpg\")):\n            t = imread(folder_path+f)\n            t_reshape = resize(t, imsize)\n            X[i, :] = t_reshape.flatten().reshape(1, flattened_size)\n            i += 1\n    \n    return (X, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## Crear dataset de entrenamiento (train set)\n\nEn este ejemplo tenemos imágenes separadas para entrenamiento (train) y prueba (test). A continuación cargamos ambos datasets, entrenamos k-NN y Naive Bayes, y posteriormente medimos el redimientos de cada clasificador utilizando distintas métricas de evaluación.\n\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "root_path = \"dogscats_dataset/train/\"\ncats_path = \"cats/\"\ndogs_path = \"dogs/\"\n\n(X_cats, Y_cats) = load_dataset(root_path+cats_path, class_index=0)\n(X_dogs, Y_dogs) = load_dataset(root_path+dogs_path, class_index=1)\n\nX_train = np.concatenate((X_cats, X_dogs))\nY_train = np.concatenate((Y_cats, Y_dogs))\n\nprint X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Crear dataset de prueba (test set)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "root_path = \"dogscats_dataset/test/\"\ncats_path = \"cats/\"\ndogs_path = \"dogs/\"\n\n(X_cats, Y_cats) = load_dataset(root_path+cats_path, class_index=0)\n(X_dogs, Y_dogs) = load_dataset(root_path+dogs_path, class_index=1)\n\nX_test = np.concatenate((X_cats, X_dogs))\nY_test = np.concatenate((Y_cats, Y_dogs))\n\nprint X_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nneigh = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\nneigh.fit(X_train, Y_train)\n\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(X_train, Y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Testing sobre el test set"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def accuracy(y, y_pred):\n    correctly_predicted = np.count_nonzero(y == y_pred)\n    accuracy = np.float(correctly_predicted) / len(y)\n    \n    return accuracy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print \"kNN Classifier\"\n\ny_pred = neigh.predict(X_test).reshape(len(Y_test),1)\n\naccuracy_score = accuracy(Y_test, y_pred)\n\nprint \"accuracy score (own implementation): \", accuracy_score\n\nfrom sklearn.metrics import accuracy_score, average_precision_score, f1_score, confusion_matrix\n\nprint \"SKLearn Metrics\"\nprint \"accuracy score: \", accuracy_score(Y_test, y_pred)\nprint \"average precision score: \", average_precision_score(Y_test, y_pred)\nprint \"f1-score: \", f1_score(Y_test, y_pred)\nprint \"Confusion matrix: \", confusion_matrix(Y_test, y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print \"Gaussian Naive Bayes Classifier\"\n\ny_pred = naive_bayes.predict(X_test).reshape(len(Y_test),1)\n\naccuracy_score = accuracy(Y_test, y_pred)\n\nprint \"accuracy score (own implementation): \", accuracy_score\n\nfrom sklearn.metrics import accuracy_score, average_precision_score, f1_score, confusion_matrix\n\nprint \"SKLearn Metrics\"\nprint \"accuracy score: \", accuracy_score(Y_test, y_pred)\nprint \"average precision score: \", average_precision_score(Y_test, y_pred)\nprint \"f1-score: \", f1_score(Y_test, y_pred)\nprint \"Confusion matrix: \", confusion_matrix(Y_test, y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#image = imread(\"dogscats_dataset/train/cats/cat.0.jpg\")\nimage = imread(\"dogscats_dataset/test/dogs/dog.110.jpg\")\n\nplt.imshow(image)\nplt.show()\n\nx = resize(image, (64,64)).reshape(1, 12288)\n\ny_hat = int(neigh.predict(x)[0])\n\nclasses = [\"cat\", \"dog\"]\n\nprint(\"It's a: \",classes[y_hat])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "<hr>\n## Validación cruzada.\n\nSi bien el particionamiento del dataset en 70-30 o 80-20 es útil al momento de validar el rendimiento del modelo, no podemos asegurar que los datos en cada partición sean **representativos**.\n\nEs por ello que una estrategia consiste en realizar una validación cruzada la cual realiza el proceso de particionamiento múltiples veces.\n\nA continuación implementamos **k-fold** y **leave-one-out**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold\n\n# Concatenemos el dataset TRAIN y el dataset TEST en un solo macro dataset X, Y\nX = np.concatenate((X_test, X_train))\nY = np.concatenate((Y_test, Y_train))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "splits = 5\nkf = KFold(n_splits=splits)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nneigh = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nnaive_bayes = GaussianNB()\n\naccuracy_score_NB = 0\naccuracy_score_kNN = 0\n\nfor train_index, test_index in kf.split(X):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]\n    \n    neigh.fit(X_train, Y_train)\n    naive_bayes.fit(X_train, Y_train)\n\n    y_pred_NB = naive_bayes.predict(X_test).reshape(len(Y_test),1)\n    y_pred_kNN = neigh.predict(X_test).reshape(len(Y_test), 1)\n    \n    print accuracy_score(y_pred_NB, Y_test), accuracy_score(y_pred_kNN, Y_test)\n    \n    accuracy_score_NB += accuracy_score(y_pred_NB, Y_test)\n    accuracy_score_kNN += accuracy_score(y_pred_kNN, Y_test)\n\nprint \"Mean test accuracy Naive Bayes: \", accuracy_score_NB / splits\nprint \"Mean test accuracy k-NN: \", accuracy_score_kNN / splits",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "<hr>\n## Ajuste fino de nuestro modelo\n\nAlgunos modelos de clasificación son paramétricos, lo cuál implica que el experto debe determinar el conjunto de parámetros que mejor desempeño consiguen. Una forma de encontrar dichos parámetros consiste en ejecutar múltiples experimentos de forma manual hasta conseguir un óptimo. Sin embargo, SKLearn nos permite automatizar esta búsqueda mediante **GridSearch**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'n_neighbors':[3,5,7]}]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "neigh = KNeighborsClassifier(metric='euclidean')\n\ngrid_search = GridSearchCV(neigh, param_grid=param_grid, cv=5, scoring='accuracy')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "grid_search.fit(X, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "grid_search.best_params_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "grid_search.cv_results_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
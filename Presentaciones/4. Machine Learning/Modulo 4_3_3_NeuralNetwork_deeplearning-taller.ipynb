{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Funciones de utilidad</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    '''\n",
    "    Linear activation given and input X, and layer parameters W and b.\n",
    "    '''\n",
    "    z = np.dot(W,X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    '''\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1. - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, a):\n",
    "    '''\n",
    "    Logistic loss.\n",
    "    '''\n",
    "    return -(y * np.log(a) + (1-y) * np.log(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(logloss):\n",
    "    '''\n",
    "    Cost function as mean of loss for every sample in dataset.\n",
    "    '''\n",
    "    return np.mean(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network(layers):\n",
    "    '''\n",
    "    Print network architecture defined by input dictionary 'layers'\n",
    "    '''\n",
    "\n",
    "    l = len(layers)\n",
    "    for i in range(l):\n",
    "        if(i == 0):\n",
    "            print('Input layer: ', layers['l'+str(i)])\n",
    "        elif(i < l-1):\n",
    "            print('Hidden layer: ',i,' ', layers['l'+str(i)])\n",
    "        else:\n",
    "            print('Output layer: ', layers['l'+str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {'l0':(2, 'sigmoid'), 'l1':(2, 'sigmoid'), 'l2':(3, 'sigmoid'), 'l3':(1, 'sigmoid')} #dictionary with layers parameters\n",
    "print_network(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Inicialización de parámetros</h1>\n",
    "\n",
    "Definido como:\n",
    "\n",
    "<font size=3>\n",
    "<center>$W^{[i]} \\in R^{nl^{[i]} \\times nl^{[i-1]}}$</center>\n",
    "<center>$b^{[i]} \\in R^{nl^{[i]} \\times 1}$</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(layers):\n",
    "    '''\n",
    "    Random initialization of parameters W and b for every layer in network specified in dictionary 'layers'.\n",
    "    \n",
    "    Input dictionary has the form: 'li':(int:number of neurons, string:activation function).\n",
    "    activation function can be: 'sigmoid' and 'relu'\n",
    "    '''\n",
    "    \n",
    "    l = len(layers) #number of layers\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1, l):\n",
    "        W = None\n",
    "        b = None\n",
    "        \n",
    "        parameters['l'+str(i)] = {'W':W, 'b':b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Forward propagation - propagación hacia adelante</h1>\n",
    "\n",
    "Definida como:\n",
    "\n",
    "<font size=3>\n",
    "<center>$Z^{[i]} = W^{[i]} \\cdot A^{[i-1]} + b^{[i]}$</center>\n",
    "<center>$A^{[i]} = \\sigma(Z^{i})$</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, parameters):\n",
    "    '''\n",
    "    Forward propagation given an input dataset X, and a neural network parameters in a dictionary.\n",
    "    '''\n",
    "    \n",
    "    forward_computation = {}\n",
    "    \n",
    "    A = X\n",
    "    forward_computation['l0'] = {'Z':None, 'A':X}\n",
    "    \n",
    "    l = len(parameters)\n",
    "    \n",
    "    for i in range(1,l+1):\n",
    "        W = parameters['l'+str(i)]['W']\n",
    "        b = parameters['l'+str(i)]['b']\n",
    "        Z = None\n",
    "        A = None\n",
    "        \n",
    "        forward_computation['l'+str(i)] = {'Z':Z, 'A':A}\n",
    "    \n",
    "    return forward_computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {'l0':(2, 'sigmoid'), 'l1':(2, 'sigmoid'), 'l2':(3, 'sigmoid'), 'l3':(1, 'sigmoid')} #dictionary with layers parameters\n",
    "print_network(layers)\n",
    "\n",
    "parameters = init(layers)\n",
    "print parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "X = X.T\n",
    "\n",
    "forward_computation = feed_forward(X, parameters)\n",
    "print(forward_computation)\n",
    "print len(forward_computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Backward propagation</h1>\n",
    "\n",
    "Definida como:\n",
    "\n",
    "<font size=3>\n",
    "<center>$dZ^{[i]} = A^{[i]} - Y$, si $i == l$</center>\n",
    "<center>$dZ^{[i]} = (W^{[i+1]}T \\cdot dZ^{[i+1]}) \\times \\sigma^{'}(Z^{[i]})$, en caso contrario</center>\n",
    "<br>\n",
    "<center>$dW^{[i]} = (dZ^{[i]} \\cdot A^{[i-1]}) / m$</center>\n",
    "<center>$db^{[i]} = \\sum{(dZ^{[i]})} / m$</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, forward_computation, Y):\n",
    "    '''\n",
    "    Computes derivatives for W and b for each layer in forward_computation.\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T) / m\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dZ2 = np.multiply(np.dot(W3.T, dZ3), d_sigmoid(Z2))\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m \n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), d_sigmoid(Z1))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    '''\n",
    "    \n",
    "    backward_computation = {}\n",
    "    \n",
    "    l = len(parameters)\n",
    "    m = len(Y)\n",
    "    \n",
    "    for i in reversed(range(1,l+1)):\n",
    "        #print('layer: ',i)\n",
    "        \n",
    "        if(i == l):\n",
    "            dZ = None\n",
    "        else:\n",
    "            W = parameters['l'+str(i+1)]['W']\n",
    "            Z = forward_computation['l'+str(i)]['Z']\n",
    "            dZ = None\n",
    "        \n",
    "        A = forward_computation['l'+str(i-1)]['A']\n",
    "        dW = None\n",
    "        db = None\n",
    "        \n",
    "        backward_computation['l'+str(i)] = {'dZ':dZ, 'dW':dW, 'db':db}\n",
    "        \n",
    "        \n",
    "    return backward_computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_computation = backward_propagation(parameters, forward_computation, Y)\n",
    "print backward_computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Actualización de parámetros</h1>\n",
    "\n",
    "Definida como:\n",
    "\n",
    "<font size=3>\n",
    "<center>$W^{[i]} = W^{[i]} - \\alpha dW^{[i]}$</center>\n",
    "<center>$b^{[i]} = b^{[i]} - \\alpha db^{[i]}$</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(parameters, backward_computation, learning_rate):\n",
    "    \n",
    "    l = len(parameters)\n",
    "    \n",
    "    for i in range(1,l+1):\n",
    "        parameters['l'+str(i)]['W'] = None\n",
    "        parameters['l'+str(i)]['b'] = None\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = update_params(parameters, backward_computation, 0.5)\n",
    "print parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Algunas utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UTILIDADES\n",
    "'''\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(data_type, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generate a binary dataset with distribution data_type\n",
    "\n",
    "    Arguments:\n",
    "    data_type -- distribution of dataset {moons,circles,blobs}\n",
    "\n",
    "    Returns:\n",
    "    X -- features\n",
    "    Y -- labels\n",
    "    \"\"\" \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(200, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_lr(parameters, X, Y):\n",
    "    X = X.T\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict_multilayer(parameters, np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def predict_multilayer(parameters, X):\n",
    "    forward_computation = {}\n",
    "    \n",
    "    A = X\n",
    "    \n",
    "    for i in range(len(parameters)):\n",
    "        W = parameters['l'+str(i+1)]['W']\n",
    "        b = parameters['l'+str(i+1)]['b']\n",
    "        Z = linear_activation(W,b,A)\n",
    "        A = sigmoid(Z)\n",
    "\n",
    "    return np.round(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Trabajemos\n",
    "3. Realicemos descenso del gradiente sobre la red neural completa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, Y = generate_data('moons')\n",
    "nx,m = X.T.shape\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### - Apliquemos descenso del gradiente a cada regresor logístico por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metaparameters initialization\n",
    "'''\n",
    "num_epochs = 20000\n",
    "learning_rate = 0.1\n",
    "\n",
    "'''\n",
    "Dataset loading\n",
    "'''\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "X = X.T\n",
    "\n",
    "'''\n",
    "Parameters initialization\n",
    "'''\n",
    "layers = {'l0':(2, 'sigmoid'), 'l1':(5, 'sigmoid'), 'l2':(5, 'sigmoid'), 'l3':(1, 'sigmoid')} #dictionary with layers parameters\n",
    "parameters = init(layers)\n",
    "\n",
    "l = len(layers)\n",
    "\n",
    "error_array = np.zeros([num_epochs])#lo utilizaremos para plotear el error\n",
    "\n",
    "'''\n",
    "Gradient descent\n",
    "'''\n",
    "for i in range(num_epochs): \n",
    "    '''\n",
    "    Forward Propagation\n",
    "    '''\n",
    "    forward_computation = None\n",
    "        \n",
    "    '''\n",
    "    Backward Propagation\n",
    "    '''\n",
    "    backward_computation = None\n",
    "    \n",
    "    '''\n",
    "    Parameters Update\n",
    "    '''\n",
    "    parameters = None\n",
    "    \n",
    "    '''\n",
    "    Cost estimation\n",
    "    '''\n",
    "    J = cost(loss(Y, forward_computation['l'+str(l-1)]['A']))\n",
    "    error_array[i] = J\n",
    "    \n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo -- iteracion \", i, \": \", J)\n",
    "        \n",
    "print(\"parametros actualizados: \", parameters)\n",
    "\n",
    "'''\n",
    "Visualizacion del error por epoca\n",
    "'''\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(0,num_epochs-1, num_epochs), error_array)\n",
    "plt.xlabel(\"numero de epocas\")\n",
    "plt.ylabel(\"error: \"+r'$J$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Visualizacion del resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_lr(parameters, X, Y):\n",
    "    X = X.T\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = predict_multilayer(parameters, np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_lr(parameters, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
